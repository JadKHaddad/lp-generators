{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis\n",
    "\n",
    "This notebook reproduces all figures from the paper using the datasets produced by running `make` in the scripts directory.\n",
    "Brief description of the datasets:\n",
    "\n",
    "* `naive_random.json` and `parameterised_random.json`:\n",
    "Store parameters, features and performance metrics for instances generated using the naive and controlled generators.\n",
    "In JSON format, each entry is an instance, with keys storing feature values.\n",
    "The complete instance data is also generated and stored by the scripts (as compressed serialised numpy matrices) so they can be used as seed points for local search runs.\n",
    "\n",
    "* `naive_search.json` and `parameterised_search.json`:\n",
    "Store feature-space search records.\n",
    "Each row records a search step.\n",
    "The scripts record the full feature and performance metric set of the current instance found by local search every 100 steps.\n",
    "\n",
    "* `naive_performance_search_*.json` and `parameterised_performance_search_*.json`:\n",
    "Store performance-space search records.\n",
    "File names indicate the performance metric used for search (e.g. `naive_performance_search_clp_primal_iterations.json`) stores the results of local search which aims to increase the number of iterations required by primal simplex to solve the instance.\n",
    "Each row records a search step.\n",
    "The scripts record the full feature and performance metric set of the current instance found by local search every 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import os\n",
    "import json\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('white')\n",
    "\n",
    "rename_columns = {\n",
    "    'var_degree_max': 'Variable Degree Max',\n",
    "    'cons_degree_max': 'Constraint Degree Max',\n",
    "    'var_degree_min': 'Variable Degree Min',\n",
    "    'cons_degree_min': 'Constraint Degree Min',\n",
    "    'coefficient_density': 'Coefficient Density',\n",
    "    'total_fractionality': 'Total Fractionality',\n",
    "    'binding_constraints': 'Number of Binding Constraints',\n",
    "    'rhs_mean': 'Constraint RHS Values Mean',\n",
    "    'obj_mean': 'Objective Coefficients Mean',\n",
    "    'clp_dual_iterations': 'Dual Simplex Iterations',\n",
    "    'clp_barrier_iterations': 'Barrier Method Iterations',\n",
    "    'clp_primal_iterations': 'Primal Simplex Iterations'\n",
    "}\n",
    "\n",
    "# Randomly generated datasets - rows are instances and columns are\n",
    "# features/parameters/performance metrics.\n",
    "df_naive_random = pd.read_json('data/naive_random.json').rename(columns=rename_columns)\n",
    "df_naive_random['Generator'] = 'Naive'\n",
    "df_param_random = pd.read_json('data/parameterised_random.json').rename(columns=rename_columns)\n",
    "df_param_random['Generator'] = 'Controlled'\n",
    "\n",
    "# Feature-space search datasets. Rows record the state of a search run at step N.\n",
    "# Each dataframe contains step records for 100 runs.\n",
    "df_naive_search = pd.read_json('data/naive_search.json').rename(columns=rename_columns)\n",
    "df_param_search = pd.read_json('data/parameterised_search.json').rename(columns=rename_columns)\n",
    "\n",
    "with contextlib.suppress(FileExistsError):\n",
    "    os.mkdir('figures')\n",
    "\n",
    "def save(name):\n",
    "    plt.savefig(f'figures/{name}.pdf')\n",
    "\n",
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive generator parameters\n",
    "\n",
    "3000 instances generated, each with a new random seed and the following parameters.\n",
    "Each 'sample' generates parameters from these distributions and runs the naive algorithm with those parameters.\n",
    "\n",
    "```py\n",
    "size_params = dict(variables=50, constraints=50)\n",
    "rhs_params = dict(\n",
    "    rhs_mean=uniform(low=-100.0, high=100.0),\n",
    "    rhs_std=uniform(low=1.0, high=30.0))\n",
    "objective_params = dict(\n",
    "    obj_mean=uniform(low=-100.0, high=100.0),\n",
    "    obj_std=uniform(low=1.0, high=10.0))\n",
    "lhs_params = dict(\n",
    "    density=uniform(low=0.0, high=1.0),\n",
    "    pv=uniform(low=0.0, high=1.0),\n",
    "    pc=uniform(low=0.0, high=1.0),\n",
    "    coeff_loc=uniform(low=-2.0, high=2.0),\n",
    "    coeff_scale=uniform(low=0.1, high=1.0))\n",
    "```\n",
    "\n",
    "## Controlled generator parameters\n",
    "\n",
    "1000 instances generated, as above, using the controlled algorithm.\n",
    "Generated more naive instances to ensure we had at least 1000 feasible, bounded instances as a comparison set.\n",
    "\n",
    "```py\n",
    "size_params = dict(variables=50, constraints=50)\n",
    "beta_params = dict(\n",
    "    basis_split=random_state.uniform(low=0.0, high=1.0))\n",
    "alpha_params = dict(\n",
    "    frac_violations=random_state.uniform(low=0.0, high=1.0),\n",
    "    beta_param=random_state.lognormal(mean=-0.2, sigma=1.8),\n",
    "    mean_primal=0,\n",
    "    std_primal=1,\n",
    "    mean_dual=0,\n",
    "    std_dual=1)\n",
    "lhs_params = dict(\n",
    "    density=random_state.uniform(low=0.0, high=1.0),\n",
    "    pv=random_state.uniform(low=0.0, high=1.0),\n",
    "    pc=random_state.uniform(low=0.0, high=1.0),\n",
    "    coeff_loc=random_state.uniform(low=-2.0, high=2.0),\n",
    "    coeff_scale=random_state.uniform(low=0.1, high=1.0))\n",
    "```\n",
    "\n",
    "`df_naive_random` and `df_param_random` store features and performance metrics of randomly generated instances.\n",
    "Each instance is a row generated using the parameters described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Controlled generator samples: {df_param_random.shape[0]}')\n",
    "print(f'Naive generator samples: {df_naive_random.shape[0]}')\n",
    "print(f'Naive generator feasible bounded samples: {df_naive_random.solvable.sum()}')\n",
    "print(f'Probability naive feasible and bounded: {df_naive_random.solvable.mean():.3f}')\n",
    "df_param_random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint Features\n",
    "\n",
    "Plots below show features of the constraint matrix, which do not differ between generators, so we can just use the results in `df_param_random`.\n",
    "Each point in the plots below represent an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    data=df_param_random,\n",
    "    x='Coefficient Density',\n",
    "    y='Variable Degree Max',\n",
    "    fit_reg=False)\n",
    "save('feature-dist-density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(\n",
    "    data=df_param_random,\n",
    "    x='Constraint Degree Max',\n",
    "    y='Variable Degree Max',\n",
    "    fit_reg=False)\n",
    "save('feature-dist-degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Features\n",
    "\n",
    "Distribution of solution features differs between generators because the controlled generator explictly sets the solution.\n",
    "These features are only relevant for feasible and bounded instances so the naive generator results are filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(\n",
    "    data=pd.concat([\n",
    "        df_param_random,\n",
    "        df_naive_random[df_naive_random.solvable]]),\n",
    "    x='Number of Binding Constraints',\n",
    "    y='Total Fractionality',\n",
    "    hue='Generator',\n",
    "    hue_order=['Controlled', 'Naive'],\n",
    "    palette=reversed(palette[0:2]),\n",
    "    fit_reg=False)\n",
    "ax.ax.set_ybound(lower=-1, upper=26)\n",
    "save('feature-dist-relaxation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [ 0.,  5., 10., 15., 20., 25., 30., 35., 40., 45., 50.]\n",
    "df_param_random[\"Number of Binding Constraints\"].hist(\n",
    "    bins=bins, alpha=0.5)\n",
    "df_naive_random[df_naive_random.solvable][\"Number of Binding Constraints\"].hist(\n",
    "    bins=bins, alpha=0.5)\n",
    "plt.gca().grid(False)\n",
    "sns.despine()\n",
    "plt.legend([\"Controllable\", \"Naive\"])\n",
    "plt.xlabel(\"Number of Binding Constraints\")\n",
    "plt.ylabel(\"Number of Generated Instances\")\n",
    "save('feature-dist-binding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective/RHS Features\n",
    "\n",
    "Plot below compares feasible and bounded instances from both generators by their positions in objective/rhs feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(\n",
    "    data=pd.concat([\n",
    "        df_param_random,\n",
    "        df_naive_random[df_naive_random.solvable]]),\n",
    "    x='Constraint RHS Values Mean',\n",
    "    y='Objective Coefficients Mean',\n",
    "    hue='Generator',\n",
    "    hue_order=['Naive', 'Controlled'],\n",
    "    palette=palette[0:2],\n",
    "    fit_reg=False)\n",
    "save('feature-dist-obj-rhs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feasibility Phase Transition\n",
    "\n",
    "The naive generator produces instances where the mean values of objective and rhs coefficients are uniformly distributed.\n",
    "The plots below show feasibility and boundedness by position in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_naive_random['Feasible and Bounded'] = df_naive_random['solvable'].apply(\n",
    "    lambda v: 'Yes' if v else 'No')\n",
    "sns.lmplot(\n",
    "    data=df_naive_random,\n",
    "    x='Constraint RHS Values Mean',\n",
    "    y='Objective Coefficients Mean',\n",
    "    hue='Feasible and Bounded',\n",
    "    hue_order=['Yes', 'No'],\n",
    "    palette=palette[2:],\n",
    "    size=5, fit_reg=False, scatter_kws={'alpha': 0.6})\n",
    "save('naive-transition-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_naive_random[df_naive_random.solvable]\n",
    "sns.jointplot(\n",
    "    x=df['Constraint RHS Values Mean'],\n",
    "    y=df['Objective Coefficients Mean'],\n",
    "    kind='kde', color='k', size=5, stat_func=None)\n",
    "save('naive-transition-prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generated Instance Difficulty\n",
    "\n",
    "Plots below indicate difficulty of instances for dual simplex in objective/rhs feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_naive_random[df_naive_random.solvable].sample(1000)\n",
    "print(f'Naive Generator. F+B samples: {df.shape[0]}')\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(\n",
    "    x=df['Constraint RHS Values Mean'],\n",
    "    y=df['Objective Coefficients Mean'],\n",
    "    c=df['Dual Simplex Iterations'], cmap=cm.coolwarm)\n",
    "ax.set_xbound([-125, 125])\n",
    "ax.set_ybound([-125, 125])\n",
    "ax.set_xlabel('Constraint RHS Values Mean')\n",
    "ax.set_ylabel('Objective Coefficients Mean')\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('Dual Simplex Iterations')\n",
    "sns.despine()\n",
    "save('hardness-naive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_param_random[df_param_random.solvable]\n",
    "print(f'Parameterised Generator. Samples: {df.shape[0]}')\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(\n",
    "    x=df['Constraint RHS Values Mean'],\n",
    "    y=df['Objective Coefficients Mean'],\n",
    "    c=df['Dual Simplex Iterations'], cmap=cm.coolwarm)\n",
    "ax.set_xbound([-125, 125])\n",
    "ax.set_ybound([-125, 125])\n",
    "ax.set_xlabel('Constraint RHS Values Mean')\n",
    "ax.set_ylabel('Objective Coefficients Mean')\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('Dual Simplex Iterations')\n",
    "sns.despine()\n",
    "save('hardness-controlled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Difficulty\n",
    "\n",
    "Shows instance difficulty achieved by the controllable generator as a direct result of setting the number of binding constraints in generated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000)\n",
    "df2 = df_param_random[df_param_random.solvable]\n",
    "df1['Generator'] = 'Naive'\n",
    "df2['Generator'] = 'Controlled'\n",
    "sns.lmplot(\n",
    "    data=pd.concat([df1, df2]),\n",
    "    x='Number of Binding Constraints',\n",
    "    y='Dual Simplex Iterations',\n",
    "    hue='Generator',\n",
    "    hue_order=['Controlled', 'Naive'],\n",
    "    palette=reversed(palette[0:2]),\n",
    "    fit_reg=False,\n",
    "    scatter_kws={'alpha': 0.4, 'linewidths': 0, 's': 60})\n",
    "save('binding-constraints-hardness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching feature space\n",
    "\n",
    "In all runs, the objective is simply to reach the target point (-100, 100) in the obj/rhs feature space (our missing region).\n",
    "A start point is selected by randomly sampling 10 instances from the existing feasible-bounded instance set and choosing the closest point of those to the target.\n",
    "At each local search step:\n",
    "* generates a neighbour (using either naive or controllable method)\n",
    "* accepts that neighbour if it is an improvement.\n",
    "\n",
    "If the naive operator produces an infeasible or unbounded instance, it is rejected.\n",
    "Search continues for 10000 of these steps and we repeat each search process 100 times.\n",
    "The figure below shows state of play after 1000 steps.\n",
    "Subsequent figure shows reduced distance to target after N steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000).copy()\n",
    "df1['Source'] = 'Naive Generator'\n",
    "df2 = df_param_random[df_param_random.solvable].copy()\n",
    "df2['Source'] = 'Controlled Generator'\n",
    "\n",
    "step = 1000\n",
    "df3 = df_naive_search[df_naive_search.solvable & (df_naive_search.step == step)].copy()\n",
    "df3['Source'] = 'Naive Search'\n",
    "df4 = df_param_search[df_param_search.solvable & (df_param_search.step == step)].copy()\n",
    "df4['Source'] = 'Controlled Search'\n",
    "\n",
    "g = sns.lmplot(\n",
    "    data=pd.concat([df1, df2, df3, df4]),\n",
    "    x='Constraint RHS Values Mean',\n",
    "    y='Objective Coefficients Mean',\n",
    "    hue='Source',\n",
    "    fit_reg=False,\n",
    "    hue_order=[\n",
    "        'Naive Generator', 'Controlled Generator',\n",
    "        'Naive Search', 'Controlled Search'],\n",
    "    palette=palette\n",
    ")\n",
    "g.ax.set_xbound([-125, 125])\n",
    "g.ax.set_ybound([-125, 125])\n",
    "save('feature-search-target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_objective(df):\n",
    "    objective = (\n",
    "        (df['Constraint RHS Values Mean'] + 100) ** 2 +\n",
    "        (df['Objective Coefficients Mean'] - 100) ** 2) ** 0.5\n",
    "    return pd.DataFrame(dict(step=df.step, objective=objective))\n",
    "\n",
    "def compare_trajectory(labelled_series, ax, palette):\n",
    "    ''' Expects series (step, objective) for comparison. '''\n",
    "    assert len(labelled_series.items()) == 2\n",
    "    for (label, data), color, linestyle in zip(labelled_series.items(), palette, ['--', '-']):\n",
    "        df = data.groupby('step').objective.apply(\n",
    "            lambda x: {\n",
    "                'median': x.median(),\n",
    "                'lower': x.quantile(0.25),\n",
    "                'upper': x.quantile(0.75)}\n",
    "            ).unstack(1).reset_index()\n",
    "        ax.fill_between(df['step'], df['lower'], df['upper'], alpha=0.3, color=color)\n",
    "        ax.plot(df['step'], df['median'], linestyle, color=color, label=label)\n",
    "    ax.collections[0].set_hatch('xx')\n",
    "    ax.set_xlabel('Search Step')\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = compare_trajectory({\n",
    "    'Naive': calc_objective(df_naive_search),\n",
    "    'Controlled': calc_objective(df_param_search)\n",
    "    }, ax, palette)\n",
    "ax.semilogy()\n",
    "ax.set_ylabel('Distance to Target')\n",
    "sns.despine(fig)\n",
    "save('feature-search-progression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Difficulty\n",
    "\n",
    "Distributions below compare the original naive and controlled generator datasets compared with the sets of instances found at the 10000th search step of each of the 100 feature space search processes by each operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000).copy()\n",
    "df2 = df_param_random[df_param_random.solvable].copy()\n",
    "\n",
    "step = 10000\n",
    "df3 = df_naive_search[df_naive_search.solvable & (df_naive_search.step == step)].copy()\n",
    "df4 = df_param_search[df_param_search.solvable & (df_param_search.step == step)].copy()\n",
    "\n",
    "key = 'Dual Simplex Iterations'\n",
    "sns.boxplot(data=pd.DataFrame({\n",
    "    'Naive\\nGenerator': df1[key].reset_index(drop=True),\n",
    "    'Controlled\\nGenerator': df2[key].reset_index(drop=True),\n",
    "    'Naive\\nSearch': df3[key].reset_index(drop=True),\n",
    "    'Controlled\\nSearch': df4[key].reset_index(drop=True),\n",
    "}), orient='h')\n",
    "plt.xlabel('Dual Simplex Iterations')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "save('feature-search-hardness-dual-boxplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'Primal Simplex Iterations'\n",
    "sns.boxplot(data=pd.DataFrame({\n",
    "    'Naive\\nGenerator': df1[key].reset_index(drop=True),\n",
    "    'Controlled\\nGenerator': df2[key].reset_index(drop=True),\n",
    "    'Naive\\nSearch': df3[key].reset_index(drop=True),\n",
    "    'Controlled\\nSearch': df4[key].reset_index(drop=True),\n",
    "}), orient='h')\n",
    "plt.xlabel('Primal Simplex Iterations')\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "save('feature-search-hardness-primal-boxplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching performance space\n",
    "\n",
    "Operates in the same manner as feature space search, where the objective is to increase the number of iterations required by a target algorithm to solve the given feasible bounded instance.\n",
    "The plots below show the search progression summarised over 100 runs with the same objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_for_field(ax, field):\n",
    "    datasets = {\n",
    "        'Naive': pd.read_json(\n",
    "            f'data/naive_performance_search_{field}.json')[['step', field]].rename(\n",
    "            columns={field: 'objective'}),\n",
    "        'Controlled': pd.read_json(\n",
    "            f'data/parameterised_performance_search_{field}.json')[['step', field]].rename(\n",
    "            columns={field: 'objective'})}\n",
    "    ax = compare_trajectory(datasets, ax, palette)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_for_field(ax, 'clp_primal_iterations')\n",
    "ax.set_ylabel('Primal Simplex Iterations')\n",
    "sns.despine(fig)\n",
    "plt.legend(loc=4)\n",
    "save('search-primal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_for_field(ax, 'clp_dual_iterations')\n",
    "ax.set_ylabel('Dual Simplex Iterations')\n",
    "sns.despine(fig)\n",
    "save('search-dual')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_for_field(ax, 'clp_barrier_iterations')\n",
    "ax.set_ylabel('Barrier Method Iterations')\n",
    "sns.despine(fig)\n",
    "save('search-barrier')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Primal Simplex Iterations', 'Dual Simplex Iterations', 'Barrier Method Iterations']\n",
    "\n",
    "df1 = df_naive_random.loc[df_naive_random.solvable, cols]\n",
    "df2 = df_param_random.loc[df_param_random.solvable, cols]\n",
    "\n",
    "result1 = pd.DataFrame({\n",
    "    ('Naive Generator', None, 'Q1'): df1.quantile(.25),\n",
    "    ('Naive Generator', None, 'Q2'): df1.median(),\n",
    "    ('Naive Generator', None, 'Q3'): df1.quantile(.75),\n",
    "    ('Controllable Generator', None, 'Q1'): df2.quantile(.25),\n",
    "    ('Controllable Generator', None, 'Q2'): df2.median(),\n",
    "    ('Controllable Generator', None, 'Q3'): df2.quantile(.75),\n",
    "}).transpose().unstack()\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _read(method, field):\n",
    "    full_df = pd.read_json(f'data/{method}_performance_search_{field}.json')\n",
    "    for step in [0, 200, 500, 1000]:\n",
    "        df = full_df.loc[full_df.step == step, field]\n",
    "        yield dict(\n",
    "            Q1=df.quantile(.25), Q2=df.median(), Q3=df.quantile(.75),\n",
    "            step=step, field=field, method=method)\n",
    "\n",
    "import itertools\n",
    "\n",
    "result = pd.DataFrame(list(itertools.chain(*(\n",
    "    _read(method, field)\n",
    "    for method, field in itertools.product(\n",
    "        ['naive', 'parameterised'],\n",
    "        ['clp_primal_iterations', 'clp_dual_iterations', 'clp_barrier_iterations']\n",
    "    )))))\n",
    "result['field'] = result['field'].map({\n",
    "    'clp_dual_iterations': 'Dual Simplex Iterations',\n",
    "    'clp_barrier_iterations': 'Barrier Method Iterations',\n",
    "    'clp_primal_iterations': 'Primal Simplex Iterations'\n",
    "})\n",
    "result['method'] = result['method'].map({\n",
    "    'naive': 'Naive Search',\n",
    "    'parameterised': 'Controllable Search'\n",
    "})\n",
    "result = result.set_index(['field', 'method', 'step']).stack().unstack(0).unstack(2)\n",
    "result = result[[\n",
    "    'Primal Simplex Iterations',\n",
    "    'Dual Simplex Iterations',\n",
    "    'Barrier Method Iterations',\n",
    "    ]]\n",
    "result = pd.concat([result, result1]).sort_index()\n",
    "result = (\n",
    "    result\n",
    "    .rename(columns=lambda col: col.replace(' Iterations', ''))\n",
    "    .astype('int')\n",
    ")\n",
    "print(result.to_latex())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add random dist values to each of these.\n",
    "\n",
    "steps = [0, 100, 1000, 10000]\n",
    "\n",
    "def _read():\n",
    "    full_df = calc_objective(df_param_search)\n",
    "    for step in steps:\n",
    "        df = full_df.loc[full_df.step == step, 'objective']\n",
    "        yield dict(method='Controllable Search', min=df.min(), median=df.median(), step=step)\n",
    "    full_df = calc_objective(df_naive_search)\n",
    "    for step in steps:\n",
    "        df = full_df.loc[full_df.step == step, 'objective']\n",
    "        yield dict(method='Naive Search', min=df.min(), median=df.median(), step=step)\n",
    "\n",
    "result = pd.DataFrame(list(_read())).set_index(['method', 'step'])\n",
    "print(result.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Solution Correctness\n",
    "\n",
    "Generates an additional figure which shows whether the design solution features were produced correctly by the controlled generator.\n",
    "The controlled generator attempts to set the number of nonzeros/binding constraints in the LP solutions.\n",
    "This value may differ if the generated constraints lead to multiple solutions for the generated instance.\n",
    "Figure below uses a rolling average to estimate probability that the solution found by simplex differs from the design solution, as a function of constraint matrix density (which affects likelihood of degeneracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parameterised_random.json') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'basis_split': instance['params']['beta_params']['basis_split'],\n",
    "        'binding_constraints': instance['binding_constraints'],\n",
    "        'nonzeros': instance['nonzeros'],\n",
    "    }\n",
    "    for instance in data\n",
    "])\n",
    "\n",
    "solution_different = (df.binding_constraints - (df.basis_split * 50).round()).abs() > 0\n",
    "(\n",
    "    pd.DataFrame({\n",
    "    'different': solution_different,\n",
    "    'nonzeros': df.nonzeros\n",
    "    }).groupby('nonzeros').different.mean().sort_index()\n",
    "    .rolling(150, center=True).mean().dropna()\n",
    ").plot()\n",
    "plt.xlabel('Number of Nonzeros')\n",
    "plt.ylabel('Pr (Solution Different)')\n",
    "plt.xlim([150, 2300])\n",
    "sns.despine()\n",
    "save('pr-solution-different')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
