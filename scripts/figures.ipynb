{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, contextlib\n",
    "with contextlib.suppress(FileExistsError):\n",
    "    os.mkdir('data')\n",
    "# subprocess.check_call(\"gsutil -m cp gs://research-data-1432/180321-lp-gen-results/*.json data/\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('white')\n",
    "\n",
    "rename_columns = {\n",
    "    'var_degree_max': 'Variable Degree Max',\n",
    "    'cons_degree_max': 'Constraint Degree Max',\n",
    "    'var_degree_min': 'Variable Degree Min',\n",
    "    'cons_degree_min': 'Constraint Degree Min',\n",
    "    'coefficient_density': 'Coefficient Density',\n",
    "    'total_fractionality': 'Total Fractionality',\n",
    "    'binding_constraints': 'Number of Binding Constraints',\n",
    "    'rhs_mean': 'Constraint RHS Values Mean',\n",
    "    'obj_mean': 'Objective Coefficients Mean',\n",
    "    'clp_dual_iterations': 'Dual Simplex Iterations',\n",
    "    'clp_barrier_iterations': 'Barrier Method Iterations',\n",
    "    'clp_primal_iterations': 'Primal Simplex Iterations'\n",
    "}\n",
    "\n",
    "df_naive_random = pd.read_json('data/naive_random.json').rename(columns=rename_columns)\n",
    "df_naive_random['Generator'] = 'Naive'\n",
    "df_param_random = pd.read_json('data/parameterised_random.json').rename(columns=rename_columns)\n",
    "df_param_random['Generator'] = 'Controlled'\n",
    "df_naive_search = pd.read_json('data/naive_search.json').rename(columns=rename_columns)\n",
    "df_param_search = pd.read_json('data/parameterised_search.json').rename(columns=rename_columns)\n",
    "df_param_random.columns\n",
    "\n",
    "with contextlib.suppress(FileExistsError):\n",
    "    os.mkdir('figures')\n",
    "\n",
    "def save(name):\n",
    "    plt.savefig(f'figures/{name}.pdf')\n",
    "\n",
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive generator parameters\n",
    "\n",
    "3000 instances generated, each with a new random seed and the following parameters.\n",
    "Each 'sample' generates parameters from these distributions and runs the naive algorithm with those parameters.\n",
    "\n",
    "```py\n",
    "size_params = dict(variables=50, constraints=50)\n",
    "rhs_params = dict(\n",
    "    rhs_mean=uniform(low=-100.0, high=100.0),\n",
    "    rhs_std=uniform(low=1.0, high=30.0))\n",
    "objective_params = dict(\n",
    "    obj_mean=uniform(low=-100.0, high=100.0),\n",
    "    obj_std=uniform(low=1.0, high=10.0))\n",
    "lhs_params = dict(\n",
    "    density=uniform(low=0.0, high=1.0),\n",
    "    pv=uniform(low=0.0, high=1.0),\n",
    "    pc=uniform(low=0.0, high=1.0),\n",
    "    coeff_loc=uniform(low=-2.0, high=2.0),\n",
    "    coeff_scale=uniform(low=0.1, high=1.0))\n",
    "```\n",
    "\n",
    "## Controlled generator parameters\n",
    "\n",
    "1000 instances generated, as above, using the controlled algorithm.\n",
    "Generated more naive instances to ensure we had at least 1000 feasible, bounded instances as a comparison set.\n",
    "\n",
    "```py\n",
    "size_params = dict(variables=50, constraints=50)\n",
    "beta_params = dict(\n",
    "    basis_split=random_state.uniform(low=0.0, high=1.0))\n",
    "alpha_params = dict(\n",
    "    frac_violations=random_state.uniform(low=0.0, high=1.0),\n",
    "    beta_param=random_state.lognormal(mean=-0.2, sigma=1.8),\n",
    "    mean_primal=0,\n",
    "    std_primal=1,\n",
    "    mean_dual=0,\n",
    "    std_dual=1)\n",
    "lhs_params = dict(\n",
    "    density=random_state.uniform(low=0.0, high=1.0),\n",
    "    pv=random_state.uniform(low=0.0, high=1.0),\n",
    "    pc=random_state.uniform(low=0.0, high=1.0),\n",
    "    coeff_loc=random_state.uniform(low=-2.0, high=2.0),\n",
    "    coeff_scale=random_state.uniform(low=0.1, high=1.0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Controlled generator samples: {df_param_random.shape[0]}')\n",
    "print(f'Naive generator samples: {df_naive_random.shape[0]}')\n",
    "print(f'Naive generator feasible bounded samples: {df_naive_random.solvable.sum()}')\n",
    "print(f'Probability naive feasible and bounded: {df_naive_random.solvable.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Results\n",
    "\n",
    "\\Cref{fig:feature_dist} shows the variance in feature values achieved by the generators for feasible, bounded instances.\n",
    "This shows generated feasible, bounded instances only.\n",
    "Separate data sets are only shown where the feature distributions differ (specifically, the same generation algorithm is used for the constraint matrix in both cases, therefore all relevant features are sampled from identical distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:feature_dist:density,fig:feature_dist:degree} show variation in features of the constraint matrix.\n",
    "By constructing (approximate) degree sequences for the variable-constraint graph the degree statistics can be varied ~independently of the density and each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    data=df_param_random,\n",
    "    x='Coefficient Density',\n",
    "    y='Variable Degree Max',\n",
    "    fit_reg=False)\n",
    "save('feature-dist-density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(\n",
    "    data=df_param_random,\n",
    "    x='Constraint Degree Max',\n",
    "    y='Variable Degree Max',\n",
    "    fit_reg=False)\n",
    "save('feature-dist-degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:feature-dist:relaxation} shows the joint distribution of number of binding constraints and total fractionality of the solutions.\n",
    "Here we see the main additional parameters of the controlled generator - we can set the structure of the primal-dual solution.\n",
    "Refer to derivations in section 4.\n",
    "By contrast, the naive generator occupies a very narrow band in this space.\n",
    "More importantly, the parameters of this generator have no bearing on these features, so we cannot target a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(\n",
    "    data=pd.concat([\n",
    "        df_param_random,\n",
    "        df_naive_random[df_naive_random.solvable]]),\n",
    "    x='Number of Binding Constraints',\n",
    "    y='Total Fractionality',\n",
    "    hue='Generator',\n",
    "    hue_order=['Controlled', 'Naive'],\n",
    "    palette=reversed(palette[0:2]),\n",
    "    fit_reg=False)\n",
    "ax.ax.set_ybound(lower=-1, upper=26)\n",
    "save('feature-dist-relaxation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:feature-dist:obj-rhs} shows the joint distribution of mean obj/rhs.\n",
    "Note that the parameters of the naive generator distribute this feature jointly uniformly, but we have omitted any infeasible or unbounded instances produced by this generator.\n",
    "With the parameter distribution we have chosen for the controlled generator, there is a strong correlation between the two features shown here.\n",
    "While the naive generator used uncorrelated data, there is a clear relationship between location in this feature space and feasibility.\n",
    "Note in our choice of parameters, there is less variance in A than in b or c.\n",
    "We limit that range to ensure we are not simply generating scaled problems...?\n",
    "\n",
    "As previously mentioned, 37% of generated instances using the naive generator were feasible.\n",
    "This particular feature space seems ideal to observe the transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(\n",
    "    data=pd.concat([\n",
    "        df_param_random,\n",
    "        df_naive_random[df_naive_random.solvable]]),\n",
    "    x='Constraint RHS Values Mean',\n",
    "    y='Objective Coefficients Mean',\n",
    "    hue='Generator',\n",
    "    hue_order=['Naive', 'Controlled'],\n",
    "    palette=palette[0:2],\n",
    "    fit_reg=False)\n",
    "save('feature-dist-obj-rhs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:naive-transition:scatter} shows the sample points of the naive generator.\n",
    "\\Cref{fig:naive-transition:prob} shows the probability that an instance generated by the naive generator, and observed to be at a given point in the feature space, is feasible and bounded.\n",
    "This shows the transitional regions.\n",
    "This estimation of probability is specific to the parameter distribution we have chosen here.\n",
    "\n",
    "\n",
    "* Figures below show the phase transition observed in instances produces by the naive generator.\n",
    "* Since the naive method generates instances by varying b and c, we vary these properties uniformly.\n",
    "* Second figure shows the probability that an instance is feasible and bounded given its location (this is an observation very specific to this distribution...)\n",
    "\n",
    "We should note that:\n",
    "* stdev is comparatively small compared to the range in the means, and is constant. This is chosen in order to observe this phase transition. Scaling both together would not yield anything of interest - at base level we need to vary the likelihood of the obj/rhs coefficients being positive or negative.\n",
    "* likewise the values in A are being varied independently.\n",
    "\n",
    "A couple of obvious observations:\n",
    "* if all b > 0, primal is zero-feasible. if all c < 0, dual is zero-feasible. In this case the instance is clearly feasible and bounded. This quadrant is likely to be uninteresting - x = 0 is the likely solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_naive_random['Feasible and Bounded'] = df_naive_random['solvable'].apply(\n",
    "    lambda v: 'Yes' if v else 'No')\n",
    "sns.lmplot(\n",
    "    data=df_naive_random,\n",
    "    x='Constraint RHS Values Mean',\n",
    "    y='Objective Coefficients Mean',\n",
    "    hue='Feasible and Bounded',\n",
    "    hue_order=['Yes', 'No'],\n",
    "    palette=palette[2:],\n",
    "    size=5, fit_reg=False, scatter_kws={'alpha': 0.6})\n",
    "save('naive-transition-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_naive_random[df_naive_random.solvable]\n",
    "sns.jointplot(\n",
    "    x=df['Constraint RHS Values Mean'],\n",
    "    y=df['Objective Coefficients Mean'],\n",
    "    kind='kde', color='k', size=5, stat_func=None)\n",
    "save('naive-transition-prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the difficulty of the naive instances - we see that that 'boring' region corresponds to easily solve instances. The transitional regions, where feasibility is less predictable, tends to produce harder FB instances.\n",
    "Furthermore, the parameterised generator, which produces instances mostly in that region, produces harder instances.\n",
    "This may be also partially explained by observing that the parameterised generator controls number of binding constranits, which seems to influence difficulty directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:hardness:naive} shows the difficulty of generated instances, by measuring number of dual simplex iterations required to solve each instance.\n",
    "Observe in the 'predictable' region, instances are extremely easy, in fact they are often solved by presolve.\n",
    "Harder instances occur in the transitional regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_naive_random[df_naive_random.solvable].sample(1000)\n",
    "print(f'Naive Generator. F+B samples: {df.shape[0]}')\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(\n",
    "    x=df['Constraint RHS Values Mean'],\n",
    "    y=df['Objective Coefficients Mean'],\n",
    "    c=df['Dual Simplex Iterations'], cmap=cm.coolwarm)\n",
    "ax.set_xbound([-125, 125])\n",
    "ax.set_ybound([-125, 125])\n",
    "ax.set_xlabel('Constraint RHS Values Mean')\n",
    "ax.set_ylabel('Objective Coefficients Mean')\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('Dual Simplex Iterations')\n",
    "sns.despine()\n",
    "save('hardness-naive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:hardness:controlled} shows the difficult of instances produced with the controlled generator.\n",
    "Since this produces instances mainly in the transitional region, instances tend to be harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_param_random[df_param_random.solvable]\n",
    "print(f'Parameterised Generator. Samples: {df.shape[0]}')\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(\n",
    "    x=df['Constraint RHS Values Mean'],\n",
    "    y=df['Objective Coefficients Mean'],\n",
    "    c=df['Dual Simplex Iterations'], cmap=cm.coolwarm)\n",
    "ax.set_xbound([-125, 125])\n",
    "ax.set_ybound([-125, 125])\n",
    "ax.set_xlabel('Constraint RHS Values Mean')\n",
    "ax.set_ylabel('Objective Coefficients Mean')\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('Dual Simplex Iterations')\n",
    "sns.despine()\n",
    "save('hardness-controlled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed if we observe the relative distributions in \\Cref{generated-hardness-hist} we see that the controllable generator produces less junk and also has a harder top-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'Dual Simplex Iterations'\n",
    "x = [\n",
    "    list(df_naive_random.loc[df_naive_random.solvable, key]),\n",
    "    list(df_param_random.loc[df_param_random.solvable, key]),\n",
    "]\n",
    "plt.hist(x, bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_naive_random['Dual Simplex Iterations'] < 10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_it(data):\n",
    "    key = 'Dual Simplex Iterations'\n",
    "    bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "    df = data.loc[data.solvable, key]\n",
    "    grouped = df.groupby(pd.cut(df, bins=bins)).count()\n",
    "    grouped.index = bins[:-1]\n",
    "    return grouped\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Naive': bin_it(df_naive_random),\n",
    "    'Controlled': bin_it(df_param_random)})\n",
    "plt.bar(x=list(df.index), height=df['Naive'], width=5)\n",
    "plt.bar(x=list(df.index + 5), height=df['Controlled'], width=5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = partial(plt.hist, density=True, alpha=0.8)\n",
    "\n",
    "df1 = df_naive_random[df_naive_random.solvable]\n",
    "df2 = df_param_random[df_param_random.solvable]\n",
    "hist(df1['Dual Simplex Iterations'], label='Naive')\n",
    "hist(df2['Dual Simplex Iterations'], label='Controlled')\n",
    "plt.xlabel('Dual Simplex Iterations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "save('generated-hardness-hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further observation of \\Cref{fig:binding-constraints-hardness}, which shows difficulty for dual simplex against number of binding constraints, indicates part of the reason for this.\n",
    "The controllable generator produces instances with more binding constraints, and this is strongly correlated with difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000)\n",
    "df2 = df_param_random[df_param_random.solvable]\n",
    "df1['Generator'] = 'Naive'\n",
    "df2['Generator'] = 'Controlled'\n",
    "sns.lmplot(\n",
    "    data=pd.concat([df1, df2]),\n",
    "    x='Number of Binding Constraints',\n",
    "    y='Dual Simplex Iterations',\n",
    "    hue='Generator',\n",
    "    hue_order=['Controlled', 'Naive'],\n",
    "    palette=reversed(palette[0:2]),\n",
    "    fit_reg=False,\n",
    "    scatter_kws={'alpha': 0.6})\n",
    "save('binding-constraints-hardness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examination of figures ... shows:\n",
    "* comparatively, the controllable generator produces more feature variation,\n",
    "* harder instances,\n",
    "* varies features correlated with difficulty (solution structure).\n",
    "\n",
    "From the point of view of generating a test set we observe two issues with the results so far:\n",
    "* We are missing some regions of feature space (we do not have examples there) for feasible bounded instances.\n",
    "To test a solver effectively we want to ensure we have varied all parameters of interest, so we should use search to explore.\n",
    "* It is likely we can find more variation in performance space.\n",
    "\n",
    "The next section uses search for both..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching feature space\n",
    "\n",
    "In all runs, the objective is simply to reach the target point (-100, 100) in the obj/rhs feature space (our missing region).\n",
    "A start point is selected by randomly sampling 10 instances from the existing FB set and choosing the closest point of those to the target.\n",
    "At each local search step:\n",
    "* generates a neighbour (using either naive or controllable method)\n",
    "* accepts that neighbour if it is an improvement.\n",
    "\n",
    "If the naive operator produces an infeasible or unbounded instance, it is rejected.\n",
    "\n",
    "Search continues for 10000 of these steps.\n",
    "\n",
    "We repeat each search process 100 times.\n",
    "\n",
    "\\Cref{feature-search-target} shows the original data set, and sampled points for each search run at step 1000 (mainly to show the target point).\n",
    "\n",
    "## Searching performance space\n",
    "\n",
    "Should do a best of 1000 for the start point - we are not choosing a difficult enough start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rundown of search - the figure below shows state of play after 1000 steps.\n",
    "Subsequent figure shows reduced distance to target after N steps, clearly showing param search is more effective.\n",
    "Histograms show that we don't really find harder instances, but we do eliminate the noise of easy instances by looking in this area where we don't expect to find this instance class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000).copy()\n",
    "df1['Source'] = 'Naive Generator'\n",
    "df2 = df_param_random[df_param_random.solvable].copy()\n",
    "df2['Source'] = 'Controlled Generator'\n",
    "\n",
    "step = 1000\n",
    "df3 = df_naive_search[df_naive_search.solvable & (df_naive_search.step == step)].copy()\n",
    "df3['Source'] = 'Naive Search'\n",
    "df4 = df_param_search[df_param_search.solvable & (df_param_search.step == step)].copy()\n",
    "df4['Source'] = 'Controlled Search'\n",
    "\n",
    "g = sns.lmplot(\n",
    "    data=pd.concat([df1, df2, df3, df4]),\n",
    "    x='Constraint RHS Values Mean',\n",
    "    y='Objective Coefficients Mean',\n",
    "    hue='Source',\n",
    "    fit_reg=False,\n",
    "    hue_order=[\n",
    "        'Naive Generator', 'Controlled Generator',\n",
    "        'Naive Search', 'Controlled Search'],\n",
    "    palette=palette\n",
    ")\n",
    "g.ax.set_xbound([-125, 125])\n",
    "g.ax.set_ybound([-125, 125])\n",
    "save('feature-search-target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{feature-search-progression} compares performance of naive and controlled search.\n",
    "Naive search has ~50% rejection rate due to producing inf/ub instances.\n",
    "Controlled search is hence more efficient for searching P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_objective(df):\n",
    "    objective = (\n",
    "        (df['Constraint RHS Values Mean'] + 100) ** 2 +\n",
    "        (df['Objective Coefficients Mean'] - 100) ** 2) ** 0.5\n",
    "    return pd.DataFrame(dict(step=df.step, objective=objective))\n",
    "\n",
    "def compare_trajectory(labelled_series, ax, palette):\n",
    "    ''' Expects series (step, objective) for comparison. '''\n",
    "    assert len(labelled_series.items()) == 2\n",
    "    for (label, data), color, linestyle in zip(labelled_series.items(), palette, ['--', '-']):\n",
    "        df = data.groupby('step').objective.apply(\n",
    "            lambda x: {\n",
    "                'median': x.median(),\n",
    "                'lower': x.quantile(0.25),\n",
    "                'upper': x.quantile(0.75)}\n",
    "            ).unstack(1).reset_index()\n",
    "        ax.fill_between(df['step'], df['lower'], df['upper'], alpha=0.3, color=color)\n",
    "        ax.plot(df['step'], df['median'], linestyle, color=color, label=label)\n",
    "    ax.collections[0].set_hatch('xx')\n",
    "    ax.set_xlabel('Search Step')\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = compare_trajectory({\n",
    "    'Naive': calc_objective(df_naive_search),\n",
    "    'Controlled': calc_objective(df_param_search)\n",
    "    }, ax, palette)\n",
    "ax.semilogy()\n",
    "ax.set_ylabel('Distance to Target')\n",
    "sns.despine(fig)\n",
    "save('feature-search-progression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\Cref{fig:feature-search-hardness} shows that the search processes generally produced non-trivial instances but did not find anything harder than what we could already generate.\n",
    "Therefore, we should also apply search to attempt to improve the top end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000).copy()\n",
    "df2 = df_param_random[df_param_random.solvable].copy()\n",
    "\n",
    "step = 10000\n",
    "df3 = df_naive_search[df_naive_search.solvable & (df_naive_search.step == step)].copy()\n",
    "df4 = df_param_search[df_param_search.solvable & (df_param_search.step == step)].copy()\n",
    "\n",
    "hist = partial(plt.hist, density=True, alpha=0.8)\n",
    "hist(df1['Dual Simplex Iterations'], label='Naive Generator')\n",
    "hist(df2['Dual Simplex Iterations'], label='Parameterised Generator')\n",
    "hist(df3['Dual Simplex Iterations'], label='Naive Search')\n",
    "hist(df4['Dual Simplex Iterations'], label='Parameterised Search')\n",
    "plt.xlabel('Dual Simplex Iterations')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "save('feature-search-hardness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_naive_random[df_naive_random.solvable].sample(1000).copy()\n",
    "df2 = df_param_random[df_param_random.solvable].copy()\n",
    "\n",
    "step = 10000\n",
    "df3 = df_naive_search[df_naive_search.solvable & (df_naive_search.step == step)].copy()\n",
    "df4 = df_param_search[df_param_search.solvable & (df_param_search.step == step)].copy()\n",
    "\n",
    "key = 'Dual Simplex Iterations'\n",
    "sns.boxplot(data=pd.DataFrame({\n",
    "    'Naive\\nGenerator': df1[key].reset_index(drop=True),\n",
    "    'Controlled\\nGenerator': df2[key].reset_index(drop=True),\n",
    "    'Naive\\nSearch': df3[key].reset_index(drop=True),\n",
    "    'Controlled\\nSearch': df4[key].reset_index(drop=True),\n",
    "}), orient='h')\n",
    "plt.xlabel('Dual Simplex Iterations')\n",
    "plt.tight_layout()\n",
    "save('feature-search-hardness-dual-boxplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'Primal Simplex Iterations'\n",
    "sns.boxplot(data=pd.DataFrame({\n",
    "    'Naive\\nGenerator': df1[key].reset_index(drop=True),\n",
    "    'Controlled\\nGenerator': df2[key].reset_index(drop=True),\n",
    "    'Naive\\nSearch': df3[key].reset_index(drop=True),\n",
    "    'Controlled\\nSearch': df4[key].reset_index(drop=True),\n",
    "}), orient='h')\n",
    "plt.xlabel('Primal Simplex Iterations')\n",
    "plt.tight_layout()\n",
    "save('feature-search-hardness-primal-boxplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_for_field(ax, field):\n",
    "    datasets = {\n",
    "        'Naive': pd.read_json(\n",
    "            f'data/naive_performance_search_{field}.json')[['step', field]].rename(\n",
    "            columns={field: 'objective'}),\n",
    "        'Controlled': pd.read_json(\n",
    "            f'data/parameterised_performance_search_{field}.json')[['step', field]].rename(\n",
    "            columns={field: 'objective'})}\n",
    "    ax = compare_trajectory(datasets, ax, palette)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_for_field(ax, 'clp_primal_iterations')\n",
    "ax.set_ylabel('Primal Simplex Iterations')\n",
    "sns.despine(fig)\n",
    "plt.legend(loc=4)\n",
    "save('search-primal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_for_field(ax, 'clp_dual_iterations')\n",
    "ax.set_ylabel('Dual Simplex Iterations')\n",
    "sns.despine(fig)\n",
    "save('search-dual')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plot_for_field(ax, 'clp_barrier_iterations')\n",
    "ax.set_ylabel('Barrier Method Iterations')\n",
    "sns.despine(fig)\n",
    "save('search-barrier')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Primal Simplex Iterations', 'Dual Simplex Iterations', 'Barrier Method Iterations']\n",
    "\n",
    "df1 = df_naive_random.loc[df_naive_random.solvable, cols]\n",
    "df2 = df_param_random.loc[df_param_random.solvable, cols]\n",
    "\n",
    "result1 = pd.DataFrame({\n",
    "    ('Naive Generator', None, 'Q1'): df1.quantile(.25),\n",
    "    ('Naive Generator', None, 'Q2'): df1.median(),\n",
    "    ('Naive Generator', None, 'Q3'): df1.quantile(.75),\n",
    "    ('Controllable Generator', None, 'Q1'): df2.quantile(.25),\n",
    "    ('Controllable Generator', None, 'Q2'): df2.median(),\n",
    "    ('Controllable Generator', None, 'Q3'): df2.quantile(.75),\n",
    "}).transpose().unstack()\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _read(method, field):\n",
    "    full_df = pd.read_json(f'data/{method}_performance_search_{field}.json')\n",
    "    for step in [0, 200, 500, 1000]:\n",
    "        df = full_df.loc[full_df.step == step, field]\n",
    "        yield dict(\n",
    "            Q1=df.quantile(.25), Q2=df.median(), Q3=df.quantile(.75),\n",
    "            step=step, field=field, method=method)\n",
    "\n",
    "import itertools\n",
    "\n",
    "result = pd.DataFrame(list(itertools.chain(*(\n",
    "    _read(method, field)\n",
    "    for method, field in itertools.product(\n",
    "        ['naive', 'parameterised'],\n",
    "        ['clp_primal_iterations', 'clp_dual_iterations', 'clp_barrier_iterations']\n",
    "    )))))\n",
    "result['field'] = result['field'].map({\n",
    "    'clp_dual_iterations': 'Dual Simplex Iterations',\n",
    "    'clp_barrier_iterations': 'Barrier Method Iterations',\n",
    "    'clp_primal_iterations': 'Primal Simplex Iterations'\n",
    "})\n",
    "result['method'] = result['method'].map({\n",
    "    'naive': 'Naive Search',\n",
    "    'parameterised': 'Controllable Search'\n",
    "})\n",
    "result = result.set_index(['field', 'method', 'step']).stack().unstack(0).unstack(2)\n",
    "result = result[[\n",
    "    'Primal Simplex Iterations',\n",
    "    'Dual Simplex Iterations',\n",
    "    'Barrier Method Iterations',\n",
    "    ]]\n",
    "result = pd.concat([result, result1]).sort_index()\n",
    "result = (\n",
    "    result\n",
    "    .rename(columns=lambda col: col.replace(' Iterations', ''))\n",
    "    .astype('int')\n",
    ")\n",
    "print(result.to_latex())\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add random dist values to each of these.\n",
    "\n",
    "steps = [0, 100, 1000, 10000]\n",
    "\n",
    "def _read():\n",
    "    full_df = calc_objective(df_param_search)\n",
    "    for step in steps:\n",
    "        df = full_df.loc[full_df.step == step, 'objective']\n",
    "        yield dict(method='Controllable Search', min=df.min(), median=df.median(), step=step)\n",
    "    full_df = calc_objective(df_naive_search)\n",
    "    for step in steps:\n",
    "        df = full_df.loc[full_df.step == step, 'objective']\n",
    "        yield dict(method='Naive Search', min=df.min(), median=df.median(), step=step)\n",
    "\n",
    "result = pd.DataFrame(list(_read())).set_index(['method', 'step'])\n",
    "print(result.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
